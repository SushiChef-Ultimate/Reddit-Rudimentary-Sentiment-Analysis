Commit 1:
    Connected to reddit API. We will be pulling from the economy subreddit

Commit 2:
    Figured out how to scrape data from reddit, put it into a list, transform it into something a databse can use, and exported it.
    The code now generates a table if it doesn't exist (only really runs the first time), and populates the table with data.
    We will be using Neon as our serverless database, since its free tier is accesible for someone in my position, and automatic scaling
    takes some responsibility from me.

Commit 3:
    Modified python file to continously populate NeonDB. Created a similar python file that deletes old info and replaces it with new info continously
    This will be useful when we eventually hit Neon's storage limit, but still want to up-to-date data.

    After further consideration, we will be automating use of these files on cloud, since populating the database locally
    can be somewhat cumbersome.

Commit 4:
    We will be utlizing Azure to continously deploy our python file, as well as have it run once a day to populate a databse.
    Created PostgreSQL and Azure Functions resource in Azure.

Commit 5:
    Intiated the switch from NeonDB to Azure PostgreSQL. Configured the file to pull secrets from the AzureDB to assemble a connection string

Commit 6:
    Tested file locally to ensure the file would append 100 records to the table. Once this was ensured, host.josn, __init__.py, and function.json 
    were created. 

    AzureDBPopulate has been set as the main file to run.

    Github has been committed, and Azure Functions has been set to "run"

Commit 7:
    After recieving repeated ImportModuleErrors, the decision was made to set the CI/CD to remote deploy.

    The Linux Premium Server has been known to be able to handle the imports of complicated libraries like Pandas and Numpy very well, especially if
    utilizing remote deploy with Oryx.

    After the changes, the function began to run properly.